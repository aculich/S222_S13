% last edited 5/1/2013 10:55 by PBS
% -*-Mode: LaTeX;-*-

%\documentclass[fleqn,handout]{beamer}
\documentclass[fleqn]{beamer}
\mode<presentation>
{
 \usetheme{CambridgeUS} % clean!
 % \usecolortheme{orchid}
  \usecolortheme{dolphin}
\setbeamercolor{block title}{use=structure,fg=white,bg=blue!60!black}
\setbeamercolor{block body}{use=structure,fg=black,bg=blue!7!white}
%\usetheme{Goettingen}
% \usetheme{Pittsburgh} % simple, but right flush titles
% \usetheme{Copenhagen}
%  \usetheme{PaloAlto}
%  \usetheme{Rochester} % very simple, no headings
 % \usetheme{Berkeley} % ugly square items
  \setbeamercovered{transparent}  % or whatever (possibly just delete it)
}
\useinnertheme{rectangles}
%\setbeamertemplate{caption}[numbered]
\setbeamerfont{caption}{size=\tiny}
\setbeamertemplate{footline}{%
\leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,left]{author in head/foot}%
    \usebeamerfont{author in head/foot}\hspace*{1em}Department of Statistics, University of California, Berkeley
  \end{beamercolorbox}}%
\vskip0pt}
% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}
\usepackage{amsmath}
\usepackage{comment}
%\usepackage{caption}
\usepackage{multirow}
\usepackage{url}

\parskip 0.66ex


\begin{document}

\title[] % (optional, use only with long paper titles)
{Predicting YouTube Comedy Slam Winners}

%\author[Author, Another] % (optional, use only with lots of authors)
\author{STAT 222 Class 2013}
\institute{{\small Department of Statistics \\University of California, Berkeley}}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author[Author, Another] % (optional, use only with lots of authors)
% - Use the \inst{?} command only if the authors have different
%   affiliation.

% \date % [Short Occasion] % (optional)
\date{7 May 2013 }
\subject{Statistical Machine Learning}


\begin{frame}

\titlepage


\end{frame}

%\begin{frame}
%\frametitle{Outline}
%{\small
%\tableofcontents
%}
% %  % You might wish to add the option [pausesections]
%\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{YouTube Comedy Slam Data}

	\begin{itemize}
	   \item from UCI ML Repository; donated by Google employee
	   \item a trial is an ordered pair of video IDs $(v_i, v_j)$
	   \item data: ordered pair $+$``left'' or ``right'' found funnier by viewer
	   \item order of the pair in each trial was random
	   \item repository has training data and test data
	   \item YouTube has metadata about videos
	   \item no data about viewers
	\end{itemize}

	\begin{beamerboxesrounded}{Goal}
	\begin{center}
	    Predict which video in a pair will be funnier, from metadata
         \end{center}
         \begin{center}
         \small{(Is it OK to use the video IDs in the prediction?)}
         \end{center}
	\end{beamerboxesrounded}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Tools}
   \begin{itemize}
        \item Github
        \item IPython notebook
        \item numpy, scipy, matplotlib, nltk, scikit-learn
   \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
 \begin{frame}
\frametitle{Descriptive statistics of training data}
      \begin{itemize}
         \item 912,969 records
         \item 18,474 distinct video IDs
         \item 267,211 distinct video ID pairs
         \item 359,874 distinct ordered pairs of video IDs
         \item right video won 51.77\% of the time. $P$-value: nil
         \item judgments often discordant: \\
                  e.g., '-iuk0PbfaHY wDx28Y2RcCI' left and right each ``funnier'' 119~times
         \item accuracy of ideal classifier for training data: 73.449\% (includes videos with no comments )
         \item for individual videos, directed graph of ``funnier than''
         \item can summarize that graph by PageRank
     \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{PageRank}
      \begin{itemize}
         \item Assign each video ID (node) in the graph a numerical value
         between $0$ to $1$, known as its \textsl{PageRank}.
         \begin{center}
         \begin{tabular}{| l |}
         \hline
         At $t=0$, $PR(p_{i};0) = \frac{1}{N}$, $N$ is the total number of nodes. \\ \hline
         $PR(p_{i};t+1) = \frac{1-d}{N} + d \sum_{p_{j} \in M(p_{i})} \frac{PR(p_{j};t)}{L(p_{j})}$ \\ \hline
         Algorithm ends when $|PR(t+1) - PR(t)| \le \epsilon$ \\ \hline
         $d$ is a damping factor, default $0.85$ in scikit-learn. \\
         \hline
         \end{tabular}
         \end{center}
     \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Acquiring metadata}
   \begin{itemize}
       \item queried YouTube with video IDs using Google APIs w/i Python
       \item speedbumps: had to throttle the requests
       \item stored ``snapshot'' data as pickled Python dict
   \end{itemize}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Prediction using metadata: Feature selection}
      \begin{itemize}
          \item used mutual information to screen potential features
          \item quantitative metadata (e.g., \#views, rating, \#raters) unhelpful
          \item comments more promising
      \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Issues with comments}
    \begin{itemize}
   	\item Data Encoding: encoded the words to ASCII format and omit the garbled words
        \item Inconsistent Spellings 
        	\begin{itemize}
		\item Correct Spelling: tried Norvig's spelling corrector, ported to AWS. \\
		(e.g.: ``speling"  $\rightarrow$ ``spelling")\\
                drawback: not accurate on this corpus\\
                  (e.g.: ``youtube'' $\rightarrow$ ``couture'', 
                    ``lol'' $\rightarrow$ ``ll'', ``haha'' $\rightarrow$ ``hata'')
                   \item Text-speak:  used RegExp to standardize words\\
		\begin{center}
        			\begin{tabular}{ | l | l |}
			\hline
			Patterns & As \\ \hline
			lol, lolll, llol, lollololl & lol \\ \hline
			ha, hahahh, ahaha, jhajha & ha \\
			\hline
			\end{tabular}
					
      		\end{center}
		\item Stemming: used Porter Stemmer from NLTK package
	\end{itemize}
       
        \item Addressing Emoticons: used RegExps to replace happy faces (e.g., ``:-]'') with 
                ``happyface'' 
                  before stripping other punctuation

    \end{itemize}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Bags of Bags of Words}
     \begin{itemize}
         \item ordered pair of videos $(v_1, v_2)$ reduced to two ``bags of bags of words''  \\
                  each comment is a bag, each video has a bag of bags

         \item features derived from bags of bags:
             \begin{itemize}
                 \item presence of a word in any comment
                 \item frequency of comments with a given word
                 \item relative frequency of comments with a given word
                 \item  logOdds of frequencies and relative frequencies
                 \item etc.
              \end{itemize}
         
         \item another derived feature: PageRank predicted by linear regression\\ 
                  using (among other things) $\arctan$(difference in LOL counts)
     
     \end{itemize}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Models}

     \begin{beamerboxesrounded}{Logistic regression with Log Bayes Factor}
         Binary Output (Left v.s. Right) as response variable\\
         log bayes factor of the two ordered ids and a constant term as features
     \end{beamerboxesrounded}

     \begin{beamerboxesrounded}{Logistic regression Wenchang}
         Explain
     \end{beamerboxesrounded}

     \begin{beamerboxesrounded}{CART}
         Explain
      \end{beamerboxesrounded}
     
      \begin{beamerboxesrounded}{Page Rank}
         Explain
     \end{beamerboxesrounded}

\end{frame}
%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Features for Logistic Regression 1}	
 

      \begin{beamerboxesrounded}{Log Bayes Factors}     
      \begin{enumerate}\vspace{2mm}
      \item determine whether video $v_i$ won more than it lost, or vice versa.\\
               if $v_i$ won more than lost, it's a ``winner''\\
               if $v_i$ lost more than won, it's a``loser''
               \vspace{2mm}
      \item for each word $w$, find: \\
              $\mathbb{P}_1(\mathnormal{w})$ 
             = percentage of winner comments that contain $w$\\
             $\mathbb{P}_0(\mathnormal{w})$ = 
               percentage of loser comments that contain $w$\\
               (pad to avoid zeros)\\
               (Only used comments of "significant" winners/losers)\vspace{2mm}
      \item derive new feature: \\      
$logOdds(v) =\sum_{\mathnormal{w}} \mathnormal{m_w}\log(\frac{\mathbb{P}_1(\mathnormal{w})}{\mathbb{P}_0(\mathnormal{w})}) + (\mathnormal{n-m_w})\log(\frac{1-\mathbb{P}_1(\mathnormal{w})}{1-\mathbb{P}_0(\mathnormal{w})})$\vspace{3mm}

$\mathnormal{n}$: number of comments on video $v$\\
$\mathnormal{m_w}$: number of comments on video $v$ containing $\mathnormal{w}.$  \vspace{3mm}
%\end{equation}
      \end{enumerate}
    \end{beamerboxesrounded}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Features for Logistic Regression 2}
Wenchang please fill in this slide.

\end{frame}

%----------------------------------------------------------------------------------

\begin{frame}
\frametitle{Performance on the training set}

    \begin{beamerboxesrounded}{Common criterion to measure performances}

     score= $\frac{Number of Correct Prediction}{Number of Predictable Pairs}$
     \end{beamerboxesrounded}
    \begin{itemize}
          \item Logistic with bayes odds: 53.3\%
    \end{itemize}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{The Test data}
    \begin{itemize}
        \item 225,593 records
        \item 75,447 distinct ordered pairs of videos
        \item [how many of those ordered pairs are not in the training data?]
        \item Accuracy of the ideal classifier: 0.6946 (excluding pairs with no comments)
        \item right video won: 51.62\% of the time
    \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Performance on test data}
    \begin{itemize}
        \item Bayes classifier from training data applied to test data
        \item logistic regression
        \item CART
    \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
 %----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Conclusions}
    \begin{itemize}
        \item Quantitative metadata: not informative
        \item Comments: 
        	\begin{itemize}
	   \item Logistic Regression with Bayes Classifier
	   \item Logistic Regression with LogOdds
	   \item CART
	   \item PageRank
	\end{itemize}
       \item Results
       \item Goal achieved?
    \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Improvements}
   \begin{itemize}
      \item Make better use of GitHub
      \item Having data that are of higher quality
      \item Incorporate personal preference in the dataset 
   \end{itemize}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Acknowledgement}
\begin{center}
Thanks to Philip, David and Aaron\\
for the valuable help!
\end{center}
   
\end{frame}
\end{document}
 