% last edited 5/1/2013 3:55 by PBS
% -*-Mode: LaTeX;-*-

%\documentclass[fleqn,handout]{beamer}
\documentclass[fleqn]{beamer}
\mode<presentation>
{
 \usetheme{CambridgeUS} % clean!
 % \usecolortheme{orchid}
  \usecolortheme{dolphin}
\setbeamercolor{block title}{use=structure,fg=white,bg=blue!60!black}
\setbeamercolor{block body}{use=structure,fg=black,bg=blue!7!white}
%\usetheme{Goettingen}
% \usetheme{Pittsburgh} % simple, but right flush titles
% \usetheme{Copenhagen}
%  \usetheme{PaloAlto}
%  \usetheme{Rochester} % very simple, no headings
 % \usetheme{Berkeley} % ugly square items
  \setbeamercovered{transparent}  % or whatever (possibly just delete it)
}
\useinnertheme{rectangles}
%\setbeamertemplate{caption}[numbered]
\setbeamerfont{caption}{size=\tiny}
\setbeamertemplate{footline}{%
\leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,left]{author in head/foot}%
    \usebeamerfont{author in head/foot}\hspace*{1em}Department of Statistics, University of California, Berkeley
  \end{beamercolorbox}}%
\vskip0pt}
% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}
\usepackage{amsmath}
\usepackage{comment}
%\usepackage{caption}
\usepackage{multirow}
\usepackage{url}

\parskip 0.66ex


\begin{document}

\title[] % (optional, use only with long paper titles)
{Predicting YouTube Comedy Slam Winners}

%\author[Author, Another] % (optional, use only with lots of authors)
\author{STAT 222 Class 2013}
\institute{{\small Department of Statistics \\University of California, Berkeley}}

% - Use the \inst{?} command only if the authors have different
%   affiliation.
%\author[Author, Another] % (optional, use only with lots of authors)
% - Use the \inst{?} command only if the authors have different
%   affiliation.

% \date % [Short Occasion] % (optional)
\date{7 May 2013 }
\subject{Statistical Machine Learning}


\begin{frame}

\titlepage


\end{frame}

%\begin{frame}
%\frametitle{Outline}
%{\small
%\tableofcontents
%}
% %  % You might wish to add the option [pausesections]
%\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{YouTube Comedy Slam Data}

	\begin{itemize}
	   \item from UCI ML Repository; donated by Google employee
	   \item a trial is an ordered pair of video IDs
	   \item data: ordered pair $+$``left'' or ``right'' found funnier by viewer
	   \item order of the pair in each trial was random
	   \item repository has training data and test data
	   \item YouTube has metadata about videos
	   \item no data about viewers
	\end{itemize}

	\begin{beamerboxesrounded}{Goal}
	\begin{center}
	    Predict which video in a pair will be funnier, from metadata
         \end{center}
         \begin{center}
         \small{(Is it OK to use the video IDs in the prediction?)}
         \end{center}
	\end{beamerboxesrounded}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Tools}
   \begin{itemize}
        \item Github
        \item IPython notebook
        \item numpy, scipy, matplotlib, nltk, scikit-learn
   \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
 \begin{frame}
\frametitle{Descriptive statistics of training data}
      \begin{itemize}
         \item 912,969 records
         \item 18,474 distinct videos
         \item 267,211 distinct video pairs
         \item 359,874 distinct ordered pairs of videos
         \item right video won 51.77\% of the time. $P$-value: nil
         \item judgments often discordant: \\
                  e.g., '-iuk0PbfaHY wDx28Y2RcCI' left and right each ``funnier'' 119~times
         \item accuracy of ideal classifier for training data: 73.449\% (includes videos with no comments )
         \item for individual videos, directed graph of ``funnier than''
         \item can summarize that graph by PageRank
     \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{PageRank}
      \begin{itemize}
         \item Assign each video (node) in the graph a numerical value
         between $0$ to $1$, known as its \textsl{PageRank}.
         \begin{center}
         \begin{tabular}{| l |}
         \hline
         At $t=0$, $PR(p_{i};0) = \frac{1}{N}$, $N$ is the total number of nodes. \\ \hline
         $PR(p_{i};t+1) = \frac{1-d}{N} + d \sum_{p_{j} \in M(p_{i})} \frac{PR(p_{j};t)}{L(p_{j})}$ \\ \hline
         Algorithm ends when $|PR(t+1) - PR(t)| \le \epsilon$ \\ \hline
         $d$ is a damping factor, default $0.85$ in scikit-learn. \\
         \hline
         \end{tabular}
         \end{center}
     \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Acquiring metadata}
   \begin{itemize}
       \item queried YouTube with video IDs using Google APIs w/i Python
       \item speedbumps: had to throttle the requests
       \item stored ``snapshot'' data as pickled Python dict
   \end{itemize}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Prediction using metadata: Feature selection}
      \begin{itemize}
          \item used mutual information to screen potential features
          \item quantitative metadata (e.g., \#views, rating, \#raters) unhelpful
          \item comments more promising
      \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Issues with comments}
    \begin{itemize}
   	\item Data Encoding: encoded the words to ASCII format and omit the garbled words
        \item Inconsistent Spellings 
        	\begin{itemize}
		\item Correct Spelling: tried Norvig's spelling corrector, ported to AWS. \\
		(e.g.: ``speling"  $\rightarrow$ ``spelling")\\
                drawback: not accurate on this corpus\\
                  (e.g.: ``youtube'' $\rightarrow$ ``couture'', ``lol'' $\rightarrow$ ``ll'', ``haha'' $\rightarrow$ ``hata'')
                   \item Text-speak:  used RegExp to standardize words\\
		\begin{center}
        			\begin{tabular}{ | l | l |}
			\hline
			Patterns & As \\ \hline
			lol, lolll, llol, lollololl & lol \\ \hline
			ha, hahahh, ahaha, jhajha & ha \\
			\hline
			\end{tabular}
					
      		\end{center}
		\item Stemming: used Porter Stemmer from NLTK package
	\end{itemize}
       
        \item Addressing Emoticons: used RegExps to replace happy faces (e.g., ``:-]'') with 
                ``happyface'' 
                  before stripping other punctuation

    \end{itemize}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Bags of Bags of Words}
     \begin{itemize}
         \item ordered pair of videos reduced to two ``bags of bags of words''  \\
                  each comment is a bag, each video has a bag of bags

         \item features derived from bags of bags:
             \begin{itemize}
                 \item presence of a word in any comment
                 \item frequency of comments with a given word
                 \item relative frequency of comments with a given word
                 \item  logOdds of frequencies and relative frequencies
                 \item etc.
              \end{itemize}
         
         \item another derived feature: PageRank predicted by linear regression\\ 
                  using (among other things) $\arctan$(difference in LOL counts)
     
     \end{itemize}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Classifiers}

     \begin{beamerboxesrounded}{Logistic regression}
         Explain
     \end{beamerboxesrounded}
     
     \begin{beamerboxesrounded}{CART}
         Explain
      \end{beamerboxesrounded}

\end{frame}
%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Features for Logistic Regression}	
 

      \begin{beamerboxesrounded}{Log Bayes Factors}     
      \begin{enumerate}\vspace{2mm}
      \item determine each video id to winning or losing ids' groups.\vspace{3mm}
      \item for each features, calculate $\mathbb{P}_1(\mathnormal{w})$ 
             = percentage of occurrence among the winning ids' comments and 
             $\mathbb{P}_0(\mathnormal{w})$ = 
             percentage of occurrence among the losing ids' comments.\vspace{2mm}
      \item calculate the log Bayes factor = $\mathnormal{\log}(\frac{\mathbb{P}(winner | comments)}{\mathbb{P}(loser | comments)}) $ for each video id: \vspace{3mm}
      
$logOdds=\sum_{\mathnormal{w}  \in features} \mathnormal{m_w}\log(\frac{\mathbb{P}_1(\mathnormal{w})}{\mathbb{P}_0(\mathnormal{w})}) + (\mathnormal{n-m_w})\log(\frac{1-\mathbb{P}_1(\mathnormal{w})}{1-\mathbb{P}_0(\mathnormal{w})})$\vspace{3mm}

where $\mathnormal{n}$ is the number of comments in the video and $\mathnormal{m_w}$ is the number of comments contains $\mathnormal{w}.$  \vspace{3mm}
%\end{equation}
      \end{enumerate}
    \end{beamerboxesrounded}

\end{frame}

%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------

\begin{frame}
\frametitle{Performance on the training set}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{The Test data}
    \begin{itemize}
        \item 225,593 records
        \item 75,447 distinct ordered pairs of videos
        \item [how many of those ordered pairs are not in the training data?]
        \item Accuracy of the ideal classifier: 0.6946 (excluding pairs with no comments)
        \item right video won: 51.62\% of the time
    \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Performance on test data}
    \begin{itemize}
        \item Bayes classifier from training data applied to test data
        \item logistic regression
        \item CART
    \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Conclusions}
    the problem and the data
    \begin{itemize}
        \item Hard problem: taste in comedy is personal, but no data on viewers
        \item Even Bayes classifier only gets about 70\% accuracy
        \item Surprising that ``right'' has such a big advantage
        \item 
     \end{itemize}
\end{frame}

%----------------------------------------------------------------------------------
\begin{frame}
\frametitle{Lessons learned}
   Tools \& environment (github, IPython, etc.)


\end{frame}


\end{document}
